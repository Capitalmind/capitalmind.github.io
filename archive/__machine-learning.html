<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Machine Learning - Capitalmind</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #f5f5f5;
            background: #262725;
            min-height: 100vh;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }

        .header {
            text-align: center;
            color: #f5f5f5;
            margin-bottom: 3rem;
        }

        .header h1 {
            font-size: 3rem;
            margin-bottom: 0.5rem;
            font-weight: 300;
            color: #cc785c;
        }

        .header .subtitle {
            font-size: 1.2rem;
            opacity: 0.9;
            margin-bottom: 2rem;
        }

        /* Navigation Menu */
        .nav-menu {
            display: flex;
            justify-content: center;
            gap: 1.5rem;
            flex-wrap: wrap;
            margin: 2rem 0;
        }

        .nav-menu a {
            color: #cc785c;
            text-decoration: none;
            padding: 0.75rem 1.5rem;
            border: 1px solid #cc785c;
            border-radius: 5px;
            transition: all 0.3s ease;
            font-weight: 500;
        }

        .nav-menu a:hover {
            background: #cc785c;
            color: #f5f5f5;
        }

        .nav-menu a.active {
            background: #cc785c;
            color: #f5f5f5;
        }

        .links {
            display: flex;
            justify-content: center;
            gap: 2rem;
            margin-top: 2rem;
        }

        .links a {
            color: #f5f5f5;
            font-size: 1.5rem;
            transition: transform 0.3s ease;
        }

        .links a:hover {
            transform: scale(1.2);
        }

        .card {
            background: #262725;
            border-radius: 15px;
            padding: 2rem;
            margin-bottom: 2rem;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
            transition: transform 0.3s ease;
            border: 1px solid #3a3a3a;
        }

        .card:hover {
            transform: translateY(-3px);
        }

        .card h2 {
            color: #cc785c;
            margin-bottom: 1rem;
            font-size: 1.5rem;
        }

        /* Blog-style article layout */
        .article {
            background: #262725;
            border-radius: 15px;
            padding: 2.5rem;
            margin-bottom: 3rem;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
            border: 1px solid #3a3a3a;
            transition: transform 0.3s ease;
        }

        .article:hover {
            transform: translateY(-3px);
        }

        .article-header {
            border-bottom: 2px solid #cc785c;
            padding-bottom: 1rem;
            margin-bottom: 2rem;
        }

        .article h3 {
            color: #cc785c;
            font-size: 2rem;
            margin-bottom: 0.5rem;
        }

        .article-meta {
            display: flex;
            gap: 1rem;
            color: #cc785c;
            font-size: 0.9rem;
            opacity: 0.8;
            margin-bottom: 1rem;
        }

        .article-meta span {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }

        .article-content {
            line-height: 1.8;
        }

        .article-content h4 {
            color: #cc785c;
            margin: 1.5rem 0 0.75rem 0;
            font-size: 1.3rem;
        }

        .article-content p {
            margin-bottom: 1rem;
        }

        .article-content ul, .article-content ol {
            margin: 1rem 0 1rem 2rem;
        }

        .article-content li {
            margin-bottom: 0.5rem;
        }

        .code-block {
            background: #1a1a1a;
            border: 1px solid #3a3a3a;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            overflow-x: auto;
            border-left: 4px solid #cc785c;
        }

        .algorithm-complexity {
            background: #2a2a2a;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            border-left: 4px solid #cc785c;
        }

        .algorithm-complexity h5 {
            color: #cc785c;
            margin-bottom: 0.5rem;
        }

        .tech-tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1.5rem;
        }

        .tech-tag {
            background: #f7f3f0;
            color: #744c2c;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
        }

        .intro-section {
            background: #1a1a1a;
            border-radius: 10px;
            padding: 2rem;
            margin-bottom: 3rem;
            border-left: 4px solid #cc785c;
        }

        .contact-info {
            text-align: center;
            color: #f5f5f5;
            margin-top: 2rem;
        }

        @media (max-width: 768px) {
            .header h1 {
                font-size: 2rem;
            }
            
            .container {
                padding: 1rem;
            }
            
            .article {
                padding: 1.5rem;
            }

            .article h3 {
                font-size: 1.5rem;
            }

            .nav-menu {
                gap: 0.5rem;
            }

            .nav-menu a {
                padding: 0.5rem 1rem;
                font-size: 0.9rem;
            }

            .article-meta {
                flex-direction: column;
                gap: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="header">
            <h1><i class="fas fa-brain"></i> Machine Learning</h1>
            <p class="subtitle">Educational Algorithms & AI Fundamentals</p>
            
            <!-- Navigation Menu -->
            <nav class="nav-menu">
                <a href="index.html">Home</a>
                <a href="linux-tutorials.html">>_ Linux Tutorials</a>
                <a href="machine-learning.html" class="active"><i class="fas fa-brain"></i> Machine Learning</a>
                <a href="projects.html">Projects</a>
                <a href="trading.html">Trading</a>
                <a href="research.html">Research</a>
            </nav>
            
            <div class="links">
                <a href="https://github.com/Capitalmind" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
                <a href="mailto:tech@skynode.one" title="Email"><i class="fas fa-envelope"></i></a>
                <a href="https://x.com/Alt_Samman" title="X (Twitter)" target="_blank" rel="noopener noreferrer"><i class="fab fa-x-twitter"></i></a>
            </div>
        </header>

        <div class="intro-section">
            <h2><i class="fas fa-graduation-cap"></i> Algorithm Fundamentals</h2>
            <p>
                Explore the core algorithms that power modern machine learning and AI systems. These educational articles break down complex concepts into practical, understandable components, focusing on both theoretical foundations and real-world applications.
            </p>
            <p style="margin-top: 1rem;">
                From pathfinding algorithms to classification techniques, each entry provides implementation insights, complexity analysis, and practical examples drawn from decades of hands-on experience in AI development.
            </p>
        </div>

        <!-- A* Search Algorithm Article -->
        <article class="article">
            <div class="article-header">
                <h3><i class="fas fa-route"></i> A* Search Algorithm</h3>
                <div class="article-meta">
                    <span><i class="fas fa-calendar"></i> Pathfinding & Graph Traversal</span>
                    <span><i class="fas fa-clock"></i> 8 min read</span>
                    <span><i class="fas fa-chart-line"></i> Intermediate</span>
                </div>
            </div>
            
            <div class="article-content">
                <p>
                    A* (A-star) is one of the most important and widely-used pathfinding algorithms in computer science, combining the best aspects of Dijkstra's algorithm with heuristic guidance to find optimal paths efficiently.
                </p>

                <h4>How A* Works</h4>
                <p>
                    A* maintains a priority queue of nodes to explore, using a cost function f(n) = g(n) + h(n), where g(n) is the actual cost from start to node n, and h(n) is a heuristic estimate of cost from n to the goal.
                </p>

                <div class="code-block">
def a_star(start, goal, graph, heuristic):
    open_set = PriorityQueue()
    open_set.put((0, start))
    
    came_from = {}
    g_score = {start: 0}
    f_score = {start: heuristic(start, goal)}
    
    while not open_set.empty():
        current = open_set.get()[1]
        
        if current == goal:
            return reconstruct_path(came_from, current)
        
        for neighbor in graph.neighbors(current):
            tentative_g = g_score[current] + graph.cost(current, neighbor)
            
            if neighbor not in g_score or tentative_g < g_score[neighbor]:
                came_from[neighbor] = current
                g_score[neighbor] = tentative_g
                f_score[neighbor] = tentative_g + heuristic(neighbor, goal)
                open_set.put((f_score[neighbor], neighbor))
    
    return None  # No path found
                </div>

                <h4>Key Properties</h4>
                <ul>
                    <li><strong>Admissible Heuristic:</strong> The heuristic must never overestimate the true cost to reach the goal</li>
                    <li><strong>Optimal:</strong> A* guarantees finding the shortest path when using an admissible heuristic</li>
                    <li><strong>Complete:</strong> Will always find a solution if one exists</li>
                    <li><strong>Efficient:</strong> Explores fewer nodes than uninformed search algorithms</li>
                </ul>

                <div class="algorithm-complexity">
                    <h5>Complexity Analysis</h5>
                    <p><strong>Time Complexity:</strong> O(b^d) where b is branching factor and d is depth</p>
                    <p><strong>Space Complexity:</strong> O(b^d) for storing the open and closed sets</p>
                    <p><strong>Optimality:</strong> Optimal with admissible heuristic</p>
                </div>

                <h4>Real-World Applications</h4>
                <ul>
                    <li>GPS navigation systems and route planning</li>
                    <li>Video game AI for character movement</li>
                    <li>Robotics path planning and obstacle avoidance</li>
                    <li>Network routing protocols</li>
                    <li>Puzzle solving (15-puzzle, Rubik's cube)</li>
                </ul>

                <div class="tech-tags">
                    <span class="tech-tag">Graph Theory</span>
                    <span class="tech-tag">Pathfinding</span>
                    <span class="tech-tag">Heuristics</span>
                    <span class="tech-tag">Optimization</span>
                    <span class="tech-tag">AI Search</span>
                </div>
            </div>
        </article>

        <!-- K-Nearest Neighbors Article -->
        <article class="article">
            <div class="article-header">
                <h3><i class="fas fa-project-diagram"></i> K-Nearest Neighbors (KNN)</h3>
                <div class="article-meta">
                    <span><i class="fas fa-calendar"></i> Classification & Regression</span>
                    <span><i class="fas fa-clock"></i> 6 min read</span>
                    <span class="fas fa-chart-line"></i> Beginner</span>
                </div>
            </div>
            
            <div class="article-content">
                <p>
                    K-Nearest Neighbors is one of the simplest yet most effective machine learning algorithms, making predictions based on the k closest training examples in the feature space. Despite its simplicity, KNN is surprisingly powerful for many classification and regression tasks.
                </p>

                <h4>Algorithm Overview</h4>
                <p>
                    KNN is a "lazy learning" algorithm that stores all training data and makes predictions by finding the k nearest neighbors to a query point, then using majority vote (classification) or averaging (regression) to make predictions.
                </p>

                <div class="code-block">
import numpy as np
from collections import Counter

class KNearestNeighbors:
    def __init__(self, k=3):
        self.k = k
    
    def fit(self, X, y):
        self.X_train = X
        self.y_train = y
    
    def euclidean_distance(self, x1, x2):
        return np.sqrt(np.sum((x1 - x2) ** 2))
    
    def predict(self, X):
        predictions = [self._predict(x) for x in X]
        return np.array(predictions)
    
    def _predict(self, x):
        # Calculate distances to all training points
        distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]
        
        # Get k nearest neighbors
        k_indices = np.argsort(distances)[:self.k]
        k_nearest_labels = [self.y_train[i] for i in k_indices]
        
        # Return most common class label
        most_common = Counter(k_nearest_labels).most_common(1)
        return most_common[0][0]
                </div>

                <h4>Choosing the Right K</h4>
                <ul>
                    <li><strong>K = 1:</strong> High variance, sensitive to noise, can overfit</li>
                    <li><strong>Large K:</strong> Smoother decision boundaries, may underfit</li>
                    <li><strong>Odd K:</strong> Helps avoid ties in binary classification</li>
                    <li><strong>Cross-validation:</strong> Use to find optimal k value</li>
                </ul>

                <h4>Distance Metrics</h4>
                <p>
                    The choice of distance metric significantly impacts KNN performance:
                </p>
                <ul>
                    <li><strong>Euclidean:</strong> Standard distance, works well for continuous features</li>
                    <li><strong>Manhattan:</strong> Less sensitive to outliers, good for high dimensions</li>
                    <li><strong>Hamming:</strong> Ideal for categorical or binary features</li>
                    <li><strong>Cosine:</strong> Effective for text analysis and sparse data</li>
                </ul>

                <div class="algorithm-complexity">
                    <h5>Complexity Analysis</h5>
                    <p><strong>Training Time:</strong> O(1) - just stores the data</p>
                    <p><strong>Prediction Time:</strong> O(nd) where n is training size, d is dimensions</p>
                    <p><strong>Space Complexity:</strong> O(nd) for storing training data</p>
                    <p><strong>Scalability:</strong> Can be slow with large datasets</p>
                </div>

                <h4>Advantages & Limitations</h4>
                <p><strong>Advantages:</strong></p>
                <ul>
                    <li>Simple to understand and implement</li>
                    <li>No assumptions about data distribution</li>
                    <li>Works well with small datasets</li>
                    <li>Can be used for both classification and regression</li>
                </ul>

                <p><strong>Limitations:</strong></p>
                <ul>
                    <li>Computationally expensive for large datasets</li>
                    <li>Sensitive to irrelevant features (curse of dimensionality)</li>
                    <li>Requires feature scaling for optimal performance</li>
                    <li>Storage requirements grow with dataset size</li>
                </ul>

                <h4>Practical Applications</h4>
                <ul>
                    <li>Recommendation systems (collaborative filtering)</li>
                    <li>Pattern recognition and computer vision</li>
                    <li>Text mining and document classification</li>
                    <li>Gene classification in bioinformatics</li>
                    <li>Market segmentation and customer analysis</li>
                </ul>

                <div class="tech-tags">
                    <span class="tech-tag">Classification</span>
                    <span class="tech-tag">Regression</span>
                    <span class="tech-tag">Lazy Learning</span>
                    <span class="tech-tag">Distance Metrics</span>
                    <span class="tech-tag">Non-parametric</span>
                </div>
            </div>
        </article>

        <div class="contact-info">
            <p>More algorithm deep-dives coming soon! These articles combine theoretical understanding with practical implementation insights from real-world AI development experience.</p>
        </div>
    </div>
</body>
</html>